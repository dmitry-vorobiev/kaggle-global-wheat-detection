{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "from torchvision.models.inception import BasicConv2d, InceptionA, InceptionB, InceptionC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.inception import Inception3_Encoder\n",
    "from models.style_augment import TransformNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Inception3_Encoder()\n",
    "# cp = torch.load('../../style-augmentation/styleaug/checkpoints/checkpoint_stylepredictor.pth')\n",
    "# model.load_state_dict(cp['state_dict_stylepredictor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "# x = torch.rand(6, 3, 299, 299).to(device)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TransformNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "S = 512\n",
    "\n",
    "x = torch.randn(N, 3, S, S).to(device)\n",
    "y = torch.randn(N, 100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(x, y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = torch.load('../../style-augmentation/styleaug/checkpoints/checkpoint_transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_transformer_weights(source):\n",
    "    dest = dict()\n",
    "    \n",
    "    # encoder\n",
    "    for i in range(3):\n",
    "        for name in ['weight', 'bias']:\n",
    "            name_source = 'layers.%d.conv.%s' % (i, name)\n",
    "            name_dest = 'encoder.%d.conv.%s' % (i, name)\n",
    "            dest[name_dest] = source[name_source]\n",
    "            \n",
    "    # bottleneck\n",
    "    source_layers = ['conv', 'fc_beta', 'fc_gamma']\n",
    "    dest_layers = ['layers.conv', 'beta', 'gamma']\n",
    "    \n",
    "    for i in range(5):\n",
    "        for j in range(2):\n",
    "            for source_layer, dest_layer in zip(source_layers, dest_layers):\n",
    "                for name in ['weight', 'bias']:\n",
    "                    name_source = 'layers.{}.{}.{}'.format(i + 3, source_layer + str(j +1 ), name)\n",
    "                    name_dest = 'layers.{}.conv{}.{}.{}'.format(i, j + 1, dest_layer, name)\n",
    "                    dest[name_dest] = source[name_source]\n",
    "                    \n",
    "    # decoder\n",
    "    for i in range(3):\n",
    "        for source_layer, dest_layer in zip(source_layers, dest_layers):\n",
    "                for name in ['weight', 'bias']:\n",
    "                    name_source = 'layers.{}.{}.{}'.format(i + 8, source_layer, name)\n",
    "                    name_dest = 'layers.{}.{}.{}'.format(i + 5, dest_layer, name)\n",
    "                    dest[name_dest] = source[name_source]\n",
    "                    \n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = port_transformer_weights(cp['state_dict_ghiasi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = torch.load('../../style-augmentation/styleaug/checkpoints/checkpoint_embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in cp.items():\n",
    "    print(\"{}: {}\".format(k, tuple(v.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.ones(5) + torch.randn(5) / 100\n",
    "z = torch.zeros(5) + torch.randn(5) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "\n",
    "alpha * o + (1 - alpha) * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.lerp(o, z, 1 - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.clone().lerp_(z, 1 - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleAugmentNet(nn.Module):\n",
    "    def __init__(self, img_channels=3, style_dim=100):\n",
    "        super(StyleAugmentNet, self).__init__()\n",
    "        self.style_dim = style_dim\n",
    "        self.style_encoder = Inception3_Encoder(out_features=style_dim, transform_input=True)\n",
    "        self.transform = TransformNet(img_channels)\n",
    "        \n",
    "        self.register_buffer('style_mean', torch.zeros(style_dim))\n",
    "        self.register_buffer('style_cov', torch.ones(style_dim, style_dim))\n",
    "        self.register_buffer('style_std', torch.empty(style_dim, style_dim))\n",
    "        self.compute_style_std()\n",
    "        \n",
    "    def compute_style_std(self):\n",
    "        u, s, v = torch.svd(self.style_cov)\n",
    "        s = torch.sqrt(s)\n",
    "        self.style_std = (u @ s.diag()).T\n",
    "    \n",
    "    def sample_style(self, batch_size, device=None):\n",
    "        s = torch.randn(batch_size, self.style_dim, device=device)\n",
    "        s = torch.mm(s, self.style_std).add_(self.style_mean)\n",
    "        return s\n",
    "        \n",
    "    def forward(self, x, style=None, alpha=0.5):    \n",
    "        if style is None:\n",
    "            style = self.sample_style(x.size(0), device=x.device)\n",
    "            \n",
    "        if alpha < 1:\n",
    "            x1 = F.interpolate(x, size=299, mode='bicubic', align_corners=False)\n",
    "            orig_style = self.style_encoder(x1)\n",
    "            style.lerp_(orig_style, 1 - alpha)\n",
    "            del x1\n",
    "            \n",
    "        x = self.transform(x, style)\n",
    "        return x\n",
    "    \n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        super(StyleAugmentNet, self).load_state_dict(state_dict, strict=strict)\n",
    "        self.compute_style_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StyleAugmentNet().to(device)\n",
    "# model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = torch.load('../../style-augmentation/styleaug/checkpoints/checkpoint_stylepredictor.pth')\n",
    "weights = cp['state_dict_stylepredictor']\n",
    "model.style_encoder.load_state_dict(weights)\n",
    "\n",
    "cp = torch.load('../../style-augmentation/styleaug/checkpoints/checkpoint_transformer.pth')\n",
    "weights = port_transformer_weights(cp['state_dict_ghiasi'])\n",
    "model.transform.load_state_dict(weights)\n",
    "\n",
    "cp = torch.load('../../style-augmentation/styleaug/checkpoints/checkpoint_embeddings.pth')\n",
    "weights = dict(style_mean=cp['pbn_embedding_mean'], \n",
    "               style_cov=cp['pbn_embedding_covariance'])\n",
    "model.load_state_dict(weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = model.state_dict()\n",
    "# torch.save(state, 'weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = model(x)\n",
    "# a.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:latest] *",
   "language": "python",
   "name": "conda-env-latest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
