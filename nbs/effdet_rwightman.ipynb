{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from effdet import default_detection_model_configs, load_checkpoint, load_pretrained, EfficientDet, \\\n",
    "                   DetBenchTrain\n",
    "from effdet.config.model_config import efficientdet_model_param_dict\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import WheatDataset\n",
    "from data.utils import collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('/media/dmitry/data/global-wheat-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficientdet_config(model_name='tf_efficientdet_d1'):\n",
    "    \"\"\"Get the default config for EfficientDet based on model name.\"\"\"\n",
    "    config = default_detection_model_configs()\n",
    "    config.num_classes = 1\n",
    "    model_config = efficientdet_model_param_dict[model_name]\n",
    "    return OmegaConf.merge(config, OmegaConf.create(model_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, bench_task='', pretrained=False, checkpoint_path='', \n",
    "                 checkpoint_ema=False, **kwargs):    \n",
    "    config = get_efficientdet_config(model_name)\n",
    "\n",
    "    pretrained_backbone = kwargs.pop('pretrained_backbone', True)\n",
    "    if pretrained or checkpoint_path:\n",
    "        pretrained_backbone = False  # no point in loading backbone weights\n",
    "\n",
    "    redundant_bias = kwargs.pop('redundant_bias', None)\n",
    "    if redundant_bias is not None:\n",
    "        # override config if set to something\n",
    "        config.redundant_bias = redundant_bias\n",
    "\n",
    "    model = EfficientDet(config, pretrained_backbone=pretrained_backbone, **kwargs)\n",
    "\n",
    "    # FIXME handle different head classes / anchors and re-init of necessary layers w/ pretrained load\n",
    "\n",
    "    if checkpoint_path:\n",
    "        load_checkpoint(model, checkpoint_path, use_ema=checkpoint_ema)\n",
    "    elif pretrained:\n",
    "        load_pretrained(model, config.url)\n",
    "\n",
    "    # wrap model in task specific bench if set\n",
    "    if bench_task == 'train':\n",
    "        model = DetBenchTrain(model, config)\n",
    "    elif bench_task == 'predict':\n",
    "        model = DetBenchPredict(model, config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'tf_efficientdet_d1', 'backbone_name': 'tf_efficientnet_b1', 'backbone_args': {'drop_path_rate': 0.2}, 'image_size': 640, 'num_classes': 1, 'min_level': 3, 'max_level': 7, 'num_levels': 5, 'num_scales': 3, 'aspect_ratios': [[1.0, 1.0], [1.4, 0.7], [0.7, 1.4]], 'anchor_scale': 4.0, 'pad_type': 'same', 'act_type': 'swish', 'box_class_repeats': 3, 'fpn_cell_repeats': 4, 'fpn_channels': 88, 'separable_conv': True, 'apply_bn_for_resampling': True, 'conv_after_downsample': False, 'conv_bn_relu_pattern': False, 'use_native_resize_op': False, 'pooling_type': None, 'redundant_bias': True, 'fpn_name': None, 'fpn_config': None, 'fpn_drop_path_rate': 0.0, 'alpha': 0.25, 'gamma': 1.5, 'delta': 0.1, 'box_loss_weight': 50.0, 'url': 'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d1-4c7ebaf2.pth'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_efficientdet_config('tf_efficientdet_d1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "model = create_model(\n",
    "    'tf_efficientdet_d1', \n",
    "    bench_task='train',\n",
    "    pretrained=False,\n",
    "    pretrained_backbone=True,\n",
    "    redundant_bias=None,\n",
    "    checkpoint_path=''\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 5\n",
    "# B = 5  # boxes per image\n",
    "\n",
    "# H, W = 640, 640\n",
    "\n",
    "# base = torch.tensor([0, 0, H, W])[None, None, :]\n",
    "# scale = torch.cat([torch.zeros(N, B, 2), torch.rand(N, B, 2)], dim=2)\n",
    "# shift = (torch.rand(N, B, 2) * torch.tensor([H, W])).repeat(1, 1, 2)\n",
    "\n",
    "\n",
    "# dummy_boxes = base * scale + shift\n",
    "# dummy_boxes[:, [0, 2]] = dummy_boxes[:, [0, 2]].clamp_(0, H)\n",
    "# dummy_boxes[:, [1, 3]] = dummy_boxes[:, [1, 3]].clamp_(0, W)\n",
    "# dummy_boxes = dummy_boxes.to(device)\n",
    "\n",
    "# dummy_boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(5, 3, 640, 640).to(device)\n",
    "# cls = torch.zeros(N, B).to(device)\n",
    "\n",
    "# target = dict(\n",
    "#     bbox=dummy_boxes,\n",
    "#     cls=cls  \n",
    "# )\n",
    "\n",
    "# model(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing bboxes...: 100%|██████████| 24/24 [00:00<00:00, 149.70it/s]\n"
     ]
    }
   ],
   "source": [
    "image_dir = DATA_DIR/'train'\n",
    "csv_path = DATA_DIR/'train.csv'\n",
    "\n",
    "tfms = [\n",
    "    A.Flip(),\n",
    "    A.RandomRotate90(),\n",
    "    A.Resize(640, 640, interpolation=cv2.INTER_AREA),\n",
    "    # we can send byte tensors to GPU and convert byte -> float there\n",
    "    ToTensorV2()\n",
    "]\n",
    "tfms = A.Compose(tfms, bbox_params=A.BboxParams('pascal_voc'))\n",
    "\n",
    "ds = WheatDataset(image_dir, csv_path, transforms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(3.1527, device='cuda:1', grad_fn=<AddBackward0>),\n",
       " 'class_loss': tensor(1.1541, device='cuda:1', grad_fn=<SumBackward1>),\n",
       " 'box_loss': tensor(0.0400, device='cuda:1', grad_fn=<SumBackward1>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for images, bboxes in dl:\n",
    "    images = images.to(device).float()\n",
    "    boxes, cls = [], []\n",
    "    \n",
    "    for b in bboxes:\n",
    "        c = torch.ones(len(b), device=device)\n",
    "        b = b.to(device).float()\n",
    "        boxes.append(b)\n",
    "        cls.append(c)\n",
    "    \n",
    "    target = dict(bbox=boxes, cls=cls)\n",
    "    out = model(images, target)\n",
    "    break\n",
    "    \n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:latest] *",
   "language": "python",
   "name": "conda-env-latest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
